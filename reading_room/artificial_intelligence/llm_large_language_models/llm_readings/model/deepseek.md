# DeepSeek Paper Reading

|    | date     | project              | title                                                                                                                 | link                                 | abstract                                                                                                                                                                                                                                                                                                                                       | details                                                          |
| -- | -------- | -------------------- | --------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| 1  | 20240105 | DeepSeek LLM         | DeepSeek LLM: Scaling Open-Source<br />Language Models with Longtermism                                               | [link](http://arxiv.org/abs/2401.02954) | 深入研究缩放定律，为大规模模型缩放提供指导，开发 DeepSeek LLM 项目，<br />通过收集 2 万亿标记数据集预训练，进行监督微调和直接偏好优化，创建 <br />DeepSeek Chat 模型，该模型在多个基准测试中表现出色，<br />超过 LLaMA-2 70B，在代码、数学和推理等领域优势明显。                                                                               | [details](./deepseek-paper/01-DeepSeekLLM.2401.02954v1.md)          |
| 2  | 20240111 | DeepSeek MoE         | DeepSeekMoE: Towards Ultimate Expert<br />Specialization in Mixture-of-Experts Language Models                        | [link](http://arxiv.org/abs/2401.06066) | 提出 DeepSeekMoE 架构，以实现专家的终极专业化，通过细粒度的专家分<br />割和共享专家隔离策略，提高模型的性能和效率。在训练过程中，采用多种<br />策略来验证架构的有效性，并将模型扩展到更大规模，与其他先进模型进行<br />比较，展示了其在性能和可扩展性方面的优势。                                                                              | [details](./deepseek-paper/02-DeepSeekMoE.2401.06066v1.md)          |
| 3  | 20240126 | DeepSeek Coder       | DeepSeek-Coder: When the Large Language<br />Model Meets Programming -- The Rise of Code Intelligence                 | [link](http://arxiv.org/abs/2401.14196) | 介绍 DeepSeek-Coder 系列代码模型，包括模型的训练、数据收集和处理、<br />训练策略等。通过在大规模代码数据集上的训练，DeepSeek-Coder 在代码<br />生成、代码完成和数学推理等任务上表现出色，超过了许多开源和闭源模型，<br />为开发人员提供了更强大的代码智能支持。                                                                                | [details](./deepseek-paper/03-DeepSeek-Coder.2401.14196v2.md)       |
| 4  | 20240311 | DeepSeek VL          | DeepSeek-VL: Towards Real-World<br />Vision-Language Understanding                                                    | [link](http://arxiv.org/abs/2403.05525) | 提出 DeepSeek-VL，这是一种面向现实世界视觉和语言理解的开源模型。通<br />过数据构建、模型架构和训练策略的优化，DeepSeek-VL 在视觉语言任务中<br />展现出优越的性能，能够处理图像描述、问答、推理等多种任务，为实现智<br />能的视觉和语言交互提供了有力支持。                                                                                     | [details](./deepseek-paper/04-Deepseek-vl.2403.05525v2.md)          |
| 5  | 20240523 | DeepSeek Prover      | DeepSeek-Prover: Advancing Theorem<br />Proving in LLMs through Large-Scale Synthetic Data                            | [link](http://arxiv.org/abs/2405.14333) | 通过从高中和本科数学竞赛问题中生成大量 Lean 4 证明数据，来解决语言<br />模型在定理证明方面缺乏训练数据的问题。具体方法包括将自然语言问题转<br />化为形式化陈述，过滤低质量陈述，并使用迭代证明生成来创建合成数据。<br />经过在合成数据集上的微调，DeepSeekMath 7B 模型在定理证明任务上取<br />得了显著进展。                                   | [details](./deepseek-paper/05-DeepSeek-Prover.2405.14333v1.md)      |
| 6  | 20240617 | DeepSeek Coder-V2    | DeepSeek-Coder-V2: Breaking the Barrier<br /> of Closed-Source Models in Code Intelligence                            | [link](http://arxiv.org/abs/2406.11931) | 是 DeepSeek-Coder 的进一步发展，基于 DeepSeek-V2 进行预训练，支持<br />更多编程语言和更长的上下文长度。在代码生成、代码完成、代码修复和数<br />学推理等任务上，取得了优于之前版本和闭源模型的性能，为开发人员提供<br />了更高效、更智能的代码开发工具。                                                                                        | [details](./deepseek-paper/06-DeepSeek-Coder-V2.2406.11931v1.md)    |
| 7  | 20240619 | DeepSeek V2          | DeepSeek-V2: A Strong, Economical, and Efficient<br />Mixture-of-Experts Language Model                               | [link](http://arxiv.org/abs/2405.04434) | 介绍 DeepSeek-V2，这是一个强大、经济和高效的混合专家语言模型。采用<br />多头潜在注意力（MLA）和 DeepSeekMoE 架构，在保持强大性能的同时，<br />实现了经济的训练和高效的推理。通过在高质量和多源语料库上的预训练，<br />以及后续的监督微调和强化学习，DeepSeek-V2 在多个基准测试中表现出<br />色，成为最强的开源 MoE 语言模型之一。              | [details](./deepseek-paper/07-DeepSeek-V2.2405.04434v5.md)          |
| 8  | 20240815 | DeepSeek Prover-V1.5 | DeepSeek-Prover-V1.5: Harnessing Proof Assistant<br />Feedback for Reinforcement Learning and Monte-Carlo Tree Search | [link](http://arxiv.org/abs/2408.08152) | 在 DeepSeek-Prover-V1 的基础上进行优化，通过进一步预训练、监督微<br />调和强化学习，引入 RMaxTS 蒙特卡洛树搜索算法，显著提高了定理证明<br />能力。在多个基准测试中取得了新的最先进结果，为语言模型在定理证明<br />领域的应用提供了更强大的工具。                                                                                               | [details](./deepseek-paper/08-DeepSeek-Prover-V1.5.2408.08152v1.md) |
| 9  | 20241213 | DeepSeek VL2         | DeepSeek-VL2: Mixture-of-Experts Vision-Language<br />Models for Advanced Multimodal Understanding                    | [link](http://arxiv.org/abs/2412.10302) | 是 DeepSeek-VL 的改进版本，采用动态平铺视觉编码策略和<br />DeepSeekMoE 模型，在视觉语言理解方面有了显著提升。它能够处理多<br />种视觉语言任务，在多个基准测试中取得了领先成绩，为先进的多模态理<br />解提供了更强大的模型支持。                                                                                                                | [details](./deepseek-paper/09-Deepseek-vl2.2412.10302v1.md)         |
| 10 | 20241227 | DeepSeek v3          | DeepSeek-V3 Technical Report                                                                                          | [link](http://arxiv.org/abs/2412.19437) | 具有 671B 总参数和 37B 激活令牌，采用多头潜在注意力（MLA）和<br /> DeepSeekMoE 架构，通过创新的负载平衡策略和多令牌预测训练目标，<br />在性能和效率方面取得了显著提升。在预训练、上下文扩展、监督微调、<br />强化学习等阶段进行了全面的优化和评估，在多个基准测试中表现出色，<br />超过了其他开源模型，达到了与领先闭源模型相当的性能。        | [details](./deepseek-paper/10-DeepSeek-V3.2412.19437v1.md)          |
| 11 | 20250122 | DeepSeek R1          | DeepSeek-R1: Incentivizing Reasoning Capability<br /> in LLMs via Reinforcement Learning                              | [link](http://arxiv.org/abs/2501.12948) | 通过大规模强化学习开发第一代推理模型 DeepSeek-R1-Zero，该模型在<br />推理任务中展现出强大的能力，但存在一些问题，如可读性差和语言混合。<br />为解决这些问题，引入 DeepSeek-R1，通过多阶段训练和冷启动数据，<br />使其在推理任务上的性能与 OpenAI-o1-1217 相当。此外，还探索了从 <br />DeepSeek-R1 到较小密集模型的蒸馏，提高了模型的推理能力。 | [details](./deepseek-paper/11-DeepSeek-R1.2501.12948v1.md)          |
| 12 | 20250129 | Janus-Pro            | Janus-Pro: Unified Multimodal Understanding and<br />Generation with Data and Model Scaling                           | [link](http://arxiv.org/abs/2501.17811) | 对 Janus 模型进行改进，优化训练策略，扩展训练数据，增加模型规模，<br />提升了多模态理解和文本到图像指令跟随能力。在多个基准测试中表现<br />出色，优于之前的统一多模态模型和一些任务特定模型，为多模态理解<br />和生成领域的发展提供了新的解决方案。                                                                                            | [details](./deepseek-paper/12-Janus-Pro.2501.17811v1.md)            |
