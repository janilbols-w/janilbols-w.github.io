# Training Strategies

## 1 Pretraining

## 2 Finetuning

### 2.1 PEFT - Parameter Efficient Fine Tuning

Reference: [huggingface - PEFT](https://huggingface.co/collections/PEFT/peft-papers-6573a1a95da75f987fb873ad)

* [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://huggingface.co/papers/2303.10512)
* [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://huggingface.co/papers/2205.05638)
* [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://huggingface.co/papers/2303.16199)
* [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://huggingface.co/papers/2108.06098)
* [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://huggingface.co/papers/2309.14859)
* [Low-rank Adaptation of Large Language Model Rescoring for Parameter-Efficient Speech Recognition](https://huggingface.co/papers/2309.15223)
* [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://huggingface.co/papers/2303.02861)
* [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://huggingface.co/papers/2306.07280)
* [GPT Understands, Too](https://huggingface.co/papers/2103.10385)
* [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://huggingface.co/papers/2101.00190)
* [The Power of Scale for Parameter-Efficient Prompt Tuning](https://huggingface.co/papers/2104.08691)
* [Combining Modular Skills in Multitask Learning](https://huggingface.co/papers/2202.13914)
