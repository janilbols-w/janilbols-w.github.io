
# Reference

* [The eighth vLLM meetup](https://lu.ma/zep56hui), with Google Cloud, January 22nd 2025. [[Slides]](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing)
* [The seventh vLLM meetup](https://lu.ma/h0qvrajz), with Snowflake, November 14th 2024. [[Slides]](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing)
* [The sixth vLLM meetup](https://lu.ma/87q3nvnh), with NVIDIA, September 9th 2024. [[Slides]](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing)
* [The fifth vLLM meetup](https://lu.ma/lp0gyjqr), with AWS, July 24th 2024. [[Slides]](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing)
* [The fourth vLLM meetup](https://lu.ma/agivllm), with Cloudflare and BentoML, June 11th 2024. [[Slides]](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing)
* [The third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/), with Roblox, April 2nd 2024. [[Slides]](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing)
* [The second vLLM meetup](https://lu.ma/ygxbpzhl), with IBM Research, January 31st 2024. [[Slides]](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing) [[Video (vLLM Update)]](https://youtu.be/Y0C-DUvEnZQ) [[Video (IBM Research &amp; torch.compile)]](https://youtu.be/m0dMtFLI-dg)
* [The first vLLM meetup](https://lu.ma/first-vllm-meetup), with a16z, October 5th 2023. [[Slides]](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing)

# 01 First Meetup [[Slides]](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing)

| component               | note                                                                                                                                                                                                                                                                                |
| ----------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| vLLM System Walkthrough | 1.Core component: LLMEngine<br />2.Centralized controller → Distributed workers<br />3.Scheduler prepares the requests at each step<br />4.Workers run the model with PagedAttention                                                                                          |
| vLLM Updates            | 1. Supporting new models<br />2. ⭐Optimizing performance<br />  - efficient de-tokenization (18%)<br />  - vectorized sampler (42%)<br />  - 5.1 upto 8.5 reqs/s <br />3. Ensuring correctness & robustness<br />4. Improving usability<br />5. Code gardening & refactoring |
| vLLM Roadmap            | 1. Optimizing latency<br />  - cuda graph<br />  - multi-process architecture (python process)<br />  - spectulative decoding<br />2. Better quantization support<br />3. CUDA-level optimizations                                                                               |

# 02 Second Meetup [[Slides]](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing)

| component | note                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| overview  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| updates   | 1. Supporting new models<br />  - MoE supported (Deepseek)<br />  - Optimized FusedMoE kernel<br />2. ⭐Optimizing performance (50% throughput, 40% latency in total)<br />  - paged attention v2<br />    - kernel 3.7x faster on A100, 7.4x faster on H100; <br />    - e2e 15% on H100 with LLaMA-13B<br />  - cuda graph<br />    - 41% latency reduction on A100<br />  - distributed runtime optimization<br />    - 70% throughput increase with LLaMA-70B on A100<br />  - faster all reduce kernel (one-shot all reduce)<br />    - kernel 2.6x on A100x8 GPUs<br />    - e2e 20% faster on A100x8 GPUs<br />3. New features<br />  - AMD support<br />  - Multi-LoRA support<br />  - Quantization Support<br />  - Prefix Caching<br />4. CI/CD |
| roadmap   | History<br />1. Latency optimization<br />  - ✅lightweight runtime<br />  - multi-process architecture<br />  - speculative decoding<br />2. ✅better quantization support<br />3. ✅CUDA-level optimization<br /><br />Improvement<br />1. Performance: H100 and MoE<br />2. Scheduling optimization: chunked prefill & prefill disaggregation<br />3. support activation quantization<br />4. extensible scheduler and memory manager<br />5. torch.compile support<br /><br />Features<br />1. diverse hardware support: TPU, Intel Gaudi/GPU/CPU<br />2. Multi-modal model support<br />3. Spectulative decoding<br />4. automatic prefix caching<br />5. structured output (json, regex, grammar)                                                                       |

# 03 Third Meetup [[Slides]](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing)

| component | notes                                                                                                                                                                                                                                                                                                                                         |
| --------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| overview  | Features<br />  - wide range models<br />  - wide range hardware supports<br />  - ⭐coverage of inference optimizations<br />    - Quantization<br />    - Prefix Caching<br />    - Multi-LoRA<br />    - Tensor Parallism                                                                                                       |
| update    | 1. model support<br />  - support llava, vision language models<br />2. hardware support<br />3. features<br />4. ⭐performance optimization<br />  - MoE kernels<br />  - Marlin Kernel for quantization<br />  - Spectulative decoding<br />5. ⭐refactoring<br />  - attention backend<br />  - model executor<br />6. oss community |
| roadmap   |                                                                                                                                                                                                                                                                                                                                               |
